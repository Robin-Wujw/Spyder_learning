scrapy框架

-什么是框架？
    -就是一个集成了很多功能并且具有很强通用性的一个项目模板。
-如何学习框架？
    -专门学习框架封装的各种功能的详细用法。
-什么是scrapy
    -爬虫中封装好的明星框架。 功能:高性能持久化存储，异步数据下载，高性能数据解析，分布式
-scrapy框架的基本使用
    - 创建一个工程 scrapy startproject xxxPro
    - 在spiders子目录中创建一个爬虫文件
        - scrapy genspider spiderName www.xxx.com
    - 执行工程:
        - scrapy crawl spiderName 

-scrapy数据解析

-scrapy持久化存储
    - 基于终端指令：
        -要求:只可以将parse方法的返回值存储到本地的文本文件中
        -持久化存储对应的文本文件类型有要求
        -指令: scrapy crawl xxx -o filePath
        -好处: 简洁、高效、便捷
        -缺点: 局限性强(数据只能存储到指定后缀的文本文件中)
    - 基于管道:
        -编码流程:
            -数据解析
            -在item类中定义相关的属性
            -将解析的数据封装存储到item类型的对象中
            -将item类型的对象提交给管道进行持久化存储的操作
            -在管道类的process_item中要将其接收到的item对象中存储的数据进行持久化存储操作
            -在配置文件中开启管道
        -好处：
            -通用性强。 
    - 面试题: 将爬取到的数据一份存储到本地一份存储到数据库，如何实现？
        -管道文件中一个管道类对应的是将数据存储到一个平台
        -爬虫文件提交的item只会给管道文件中第一个被执行的管道类接受
        -process_item中的return item表示将item传递给下一个即将被执行的管道类
-基于Spider的全站数据爬取 
    - 就是将网站中某版块下的全部页面对应的页面数据进行爬取和解析
    - 需求：爬取校花网中的照片名称
    - 实现方法:
        - 将所有页面的url添加到start_urls列表(不推荐)
        - 自行手动进行请求发送
            -手动请求发送:
                - yield scrapy.Request(url,callback):callback专门用于数据解析
-五大核心组件
    -引擎(Scrapy)
        -用来处理整个系统的数据流处理，触发事物(框架核心)
    -调度器(Scheduler)
        -用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回，可以想象成一个URL(抓去网页的网址或者说是链接)的优先队列，由它来决定下一个要抓取的网址是什么，同时去除重复的网址
    -下载器(Downloader)
        -用于下载网页内容，并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上)。
    -爬虫(Spiders)
        -爬虫主要是干活的，用于从特定的网页中提取自己需要的信息，即所谓的实体(item)，用户也可以从提取出来的链接让Scrapy继续抓取下一个页面。
    -项目管理(Pipeline)
        -负责处理爬虫从网页中提取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管理，并经过几个特定的次序处理数据。

- 请求传参
    - 使用场景: 如果要爬取解析的数据不在同一张页面中。（深度爬取）
    - 需求: 爬取boss的岗位名称和岗位描述

- 图片数据爬取之ImagesPipeline
    - 基于scrapy爬取字符串类型的数据和爬取图片类型数据的区别？
        - 字符串: 只需要基于xpath进行解析且提交管道进行持久化存储
        - 图片: xpath解析出图片src的属性值。单独对图片地址发起请求获取图片二进制类型的数据
    
    -ImagePipeline:
        -只需要将img的src的属性值进行解析，提交到管道，管道就会对图片的src进行请求发送获取图片的二进制类型的数据，且还会帮我们进行持久化存储。
    -需求:爬取站长素材中的高清图片
    -使用流程:
        - 数据解析(图片的地址)
        - 将存储图片地址的item提交到指定的管道类
        - 在管道文件中自定制一个基于ImagesPipeLine的一个管道类
            - get_media_request 
            - file_path
            - item_completed 
        - 在配置文件中:
            - 指定图片存储的目录： IMAGES_STORE = './imgs'
            - 指定开启的管道:自定制的管道类
-中间件
    - 下载中间件
        - 位置: 引擎和下载器之间
        - 作用: 批量拦截整个工程中发起的所有的请求和响应
        - 拦截请求:
            - UA伪装：process_request
            - 代理IP：process_exception:return request
        - 拦截响应:
            - 篡改响应数据，响应对象
            - 需求: 爬取网易新闻的新闻数据(标题和内容)
                -1.通过网易新闻的首页解析出五大板块详情页的url(没有动态加载)
                -2.每一个板块对应的新闻标题都是动态加载出来的(动态加载)
                -3.通过解析出每一条新闻详情页的url获取详情页的页面源码，解析出新闻内容
- CrawlSpider: 类，Spider的一个子类
    -全站数据爬取的方式
        - 基于Spider： 手动请求
        - 基于CrawlSpider: 全站数据爬取